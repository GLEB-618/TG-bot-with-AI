from transformers.trainer_callback import TrainerCallback
import torch, math, time
from shared import train_logger

class EvalLoggerCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics and "eval_loss" in metrics:
            eval_loss = metrics["eval_loss"]
            perplexity = math.exp(eval_loss) if eval_loss < 100 else float("inf")
            train_logger.info(f"\nEval loss: {eval_loss:.4f} | Perplexity: {perplexity:.2f}\n")

class TextGenerationCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        model = kwargs["model"]
        tokenizer = kwargs["tokenizer"]

        prompt = [{"role": "user", "content": "Ð“Ð¾Ñ‚Ð¾Ð² Ðº ÑÐµÐ³Ð¾Ð´Ð½ÑÑˆÐ½ÐµÐ¹ Ð½Ð¾Ñ‡Ð¸?"}]
        input_ids = tokenizer.apply_chat_template(prompt, return_tensors="pt", add_generation_prompt=True).to(model.device)

        with torch.no_grad():
            output = model.generate(
                input_ids,
                max_new_tokens=50,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        decoded = tokenizer.decode(output[0], skip_special_tokens=True)
        train_logger.info(f"Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ð¾ÑÐ»Ðµ ÑÐ¿Ð¾Ñ…Ð¸ {state.epoch:.0f}: {decoded}")

class TrainLoggerCallback(TrainerCallback):
    def __init__(self):
        self.start_time = None
        self.epoch_start = None

    def on_train_begin(self, args, state, control, **kwargs):
        self.start_time = time.time()
        train_logger.info("ðŸš€ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð°Ñ‡Ð°Ð»Ð¾ÑÑŒ")

    def on_epoch_begin(self, args, state, control, **kwargs):
        self.epoch_start = time.time()
        if state.epoch is not None:
            train_logger.info(f"ðŸ“š ÐÐ°Ñ‡Ð°Ð»Ð¾ ÑÐ¿Ð¾Ñ…Ð¸ {int(state.epoch) + 1}")

    def on_epoch_end(self, args, state, control, **kwargs):
        if self.epoch_start:
            epoch_duration = time.time() - self.epoch_start
            if state.epoch is not None:
                train_logger.info(f"âœ… Ð­Ð¿Ð¾Ñ…Ð° {int(state.epoch)} Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° Ð·Ð° {epoch_duration:.2f} ÑÐµÐº")

    def on_save(self, args, state, control, **kwargs):
        ckpt_path = f"{args.output_dir}/checkpoint-{state.global_step}"
        train_logger.info(f"ðŸ’¾ Ð§ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½: {ckpt_path}")

    def on_train_end(self, args, state, control, **kwargs):
        if self.start_time is not None:
            total_time = time.time() - self.start_time
            train_logger.info(f"ðŸ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ Ð·Ð° {total_time:.2f} ÑÐµÐº")

# ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð¸Ñ…
callbacks = [EvalLoggerCallback(), TextGenerationCallback()]