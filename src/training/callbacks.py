from transformers.trainer_callback import TrainerCallback
import torch, math, time
from shared import train_logger

class EvalLoggerCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics and "eval_loss" in metrics:
            eval_loss = metrics["eval_loss"]
            perplexity = math.exp(eval_loss) if eval_loss < 100 else float("inf")
            train_logger.info(f"\nEval loss: {eval_loss:.4f} | Perplexity: {perplexity:.2f}\n")

class TextGenerationCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        model = kwargs["model"]
        tokenizer = kwargs["tokenizer"]

        prompt = "<|user|>\n–ì–æ—Ç–æ–≤ –∫ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–π –Ω–æ—á–∏?\n<|assistant|>\n"
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

        with torch.no_grad():
            output = model.generate(
                input_ids,
                max_new_tokens=50,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        decoded = tokenizer.decode(output[0], skip_special_tokens=True)
        train_logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏ {state.epoch:.0f}: decoded")

class TrainLoggerCallback(TrainerCallback):
    def __init__(self):
        self.start_time = None
        self.epoch_start = None

    def on_train_begin(self, args, state, control, **kwargs):
        self.start_time = time.time()
        train_logger.info("üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª–æ—Å—å")

    def on_epoch_begin(self, args, state, control, **kwargs):
        self.epoch_start = time.time()
        if state.epoch is not None:
            train_logger.info(f"üìö –ù–∞—á–∞–ª–æ —ç–ø–æ—Ö–∏ {int(state.epoch) + 1}")

    def on_epoch_end(self, args, state, control, **kwargs):
        if self.epoch_start:
            epoch_duration = time.time() - self.epoch_start
            if state.epoch is not None:
                train_logger.info(f"‚úÖ –≠–ø–æ—Ö–∞ {int(state.epoch)} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {epoch_duration:.2f} —Å–µ–∫")

    def on_save(self, args, state, control, **kwargs):
        ckpt_path = f"{args.output_dir}/checkpoint-{state.global_step}"
        train_logger.info(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {ckpt_path}")

    def on_train_end(self, args, state, control, **kwargs):
        if self.start_time is not None:
            total_time = time.time() - self.start_time
            train_logger.info(f"üèÅ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time:.2f} —Å–µ–∫")

# –ü–æ–¥–∫–ª—é—á–∞–µ–º –∏—Ö
callbacks = [EvalLoggerCallback(), TextGenerationCallback()]